{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePcF01H6rr16"
   },
   "source": [
    "# GenAI-Camp: Day 01\n",
    "## Lesson: ML Service - Validation of User Input\n",
    "\n",
    "This lesson is intended to show you the basics of input validation using pydantic.  \n",
    "When deploying a machine learning model as a service, ensuring the quality and integrity of incoming data is just as important as the model itself. This is where data validation comes into play. One of the most powerful tools for data validation in Python is Pydantic.\n",
    "Pydantic is a data validation and settings management library that leverages Python's type annotations. It allows you to define data structures with clear, type-safe expectations and automatically validate incoming data. If the data does not match the expected format, Pydantic raises clear and descriptive errors, making it easy to pinpoint issues.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- create pydantic models\n",
    "- use pydantic models for validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxQu4eGGDrJe"
   },
   "source": [
    "### Set up the environment\n",
    "Import the necessary libraries, set constants, and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pydantic import BaseModel, Field\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check runtime environment to make sure we are running in a colab environment. \n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\") \n",
    "else:\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of ressources\n",
    "if COLAB:\n",
    "    # Clone the data repository into colab\n",
    "    !git clone https://github.com/openknowledge/workshop-genai-camp-data.git\n",
    "    ROOT_PATH = \"/content/workshop-genai-camp-data/day-01/\"\n",
    "else:\n",
    "    ROOT_PATH = \"../\"\n",
    "DATA_PATH = ROOT_PATH + \"/data\"\n",
    "MODEL_PATH = ROOT_PATH + \"/models\"\n",
    "\n",
    "IMDB_FILE = DATA_PATH + \"/imdb_dataset.csv\"\n",
    "MODEL_FILE = MODEL_PATH + \"/sentiment_model.pth\"\n",
    "TOKENIZER_FILE = MODEL_PATH + \"/tokenizer.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation functions\n",
    "\n",
    "def rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename columns of the dataframe to make them more readable.\n",
    "    \"\"\"\n",
    "\n",
    "    df_renamed = df.copy()\n",
    "    df_renamed.rename(columns={\"text\": \"review\", \"label\": \"sentiment\"}, inplace=True)\n",
    "\n",
    "    return df_renamed\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove punctuation from the text.\n",
    "    \"\"\"\n",
    "\n",
    "    translation_table = {ord(i): None for i in string.punctuation}\n",
    "    \n",
    "    return text.translate(translation_table)\n",
    "\n",
    "\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML tags from a string.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def transform_to_lowercase(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Transform a string to lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove stopwords from a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the english stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    words = text.split()\n",
    "\n",
    "    filtered_text = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "\n",
    "def stem_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Stem the text using a nltk stemmer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the stemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Stem each word\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "\n",
    "def preparation_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the DataFrame for modeling using the available preprocessing functions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy dataframe\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    df_processed['review'] = df_processed['review'].apply(remove_html_tags)\n",
    "\n",
    "    # Remove punctuations\n",
    "    df_processed['review'] = df_processed['review'].apply(remove_punctuation)\n",
    "    \n",
    "    # Transform to lowercase\n",
    "    df_processed['review'] = df_processed['review'].apply(transform_to_lowercase)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    df_processed['review'] = df_processed['review'].apply(remove_stopwords)\n",
    "\n",
    "    # Stem the text\n",
    "    df_processed['review'] = df_processed['review'].apply(stem_text)\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text: str, tokenizer: tokenizers.Tokenizer):\n",
    "  \"\"\"\n",
    "  Vectorizes the input text using a tokenizer.\n",
    "  Args:\n",
    "      text (str): The input text to be vectorized.\n",
    "      tokenizer (tokenizers.Tokenizer): The tokenizer to use for vectorization.\n",
    "  Returns:\n",
    "      np.ndarray: A vector representation of the input text.\n",
    "  \"\"\"\n",
    "  # Tokenize the text\n",
    "  encoding = tokenizer.encode(text)\n",
    "\n",
    "  # Create a vector of zeros with the size of the vocabulary\n",
    "  vector = np.zeros(tokenizer.get_vocab_size())\n",
    "\n",
    "  # Bag Of Words: Count the number of times each token appears in the text\n",
    "  for token_id in encoding.ids:\n",
    "      vector[token_id] +=1\n",
    "\n",
    "  # Normalize the vector\n",
    "  vector = vector / vector.sum()\n",
    "  return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "INPUT_DIM = 2000\n",
    "OUTOUT_DIM = 2\n",
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 0.0003\n",
    "DROPOUT_RATE = 0.7\n",
    "\n",
    "# Define the model\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.linear_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.drop_out_1 = nn.Dropout(p=dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop_out_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text: str, model: nn.Module, tokenizer: tokenizers.Tokenizer) -> tuple[int, float]: # tuple[label, probability]\n",
    "    \"\"\"\n",
    "    Predict the sentiment of a given text using the trained model.\n",
    "    Args:\n",
    "        text (str): The text to be predicted.\n",
    "        model (nn.Module): The trained model.\n",
    "        tokenizer (tokenizers.Tokenizer): The tokenizer used for vectorization.\n",
    "    Returns:\n",
    "        int: The predicted sentiment (0 or 1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess the text\n",
    "    processed_text = preparation_pipeline(pd.DataFrame({'review': [text]}))['review'][0]\n",
    "\n",
    "    # Vectorize the text\n",
    "    vector = vectorize_text(processed_text, tokenizer)\n",
    "    vector = torch.tensor(vector, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # Predict the sentiment\n",
    "    with torch.no_grad():\n",
    "        output = model(vector)\n",
    "\n",
    "    label = torch.argmax(output, dim=1).item()\n",
    "    probability = output[0][label].item()\n",
    "\n",
    "    return (label, probability)\n",
    "\n",
    "\n",
    "def print_prediction(review: str, label: int, probability: float):\n",
    "    \"\"\"\n",
    "    Print the prediction result.\n",
    "    Args:\n",
    "        review (str): The review text.\n",
    "        label (int): The predicted sentiment (0 or 1).\n",
    "        probability (float): The probability of the predicted sentiment.\n",
    "    \"\"\"\n",
    "    sentiment = \"positive\" if label == 1 else \"negative\"\n",
    "    print(f\"Review: {review}\\nPredicted Sentiment: {sentiment} ({probability:.2f})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model: nn.Module, filename: str):\n",
    "    \"\"\"\n",
    "    Load the model state from a file.\n",
    "    Args:\n",
    "        model (nn.Module): The model to be loaded.\n",
    "        filename (str): The name of the file to load the checkpoint from.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model state has been restored from {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Pydantic](https://docs.pydantic.dev/latest/)\n",
    "Pydantic is a data validation and parsing library for Python that leverages Python's type annotations. It provides powerful data validation using Python's standard type hints, making it easier to enforce data types and handle errors gracefully. Pydantic models are simple classes that define attributes and their types, and Pydantic ensures that the data conforms to those types. It's commonly used with FastAPI for request validation but is versatile enough for any data modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a standard python class, which does NOT use the pydantic library\n",
    "class User:\n",
    "    def __init__(self, id: int, name: str, tags: list[str]):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.tags = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating with wrong types does not raise any errors. This is bad!\n",
    "user = User(id=\"123\", name=789, tags=\"not-a-list\")\n",
    "print(user.id, user.name, user.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic solves this problem\n",
    "\n",
    "# All we need to do is to inherit from BaseModel\n",
    "class UserModel(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    tags: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating with wrong types (raises a clear error)\n",
    "try:\n",
    "    user = UserModel(id=\"123\", name=789, tags=\"not-a-list\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create more detailed validation using pydantics Field\n",
    "class UserModelWithValidation(BaseModel):\n",
    "    id: int = Field(gt=0, description=\"The ID of the user, must be greater than 0\")\n",
    "    name: str = Field(min_length=3, max_length=50, description=\"The name of the user\")\n",
    "    tags: list[str] = Field(min_items=1, description=\"A list of tags associated with the user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    user = UserModelWithValidation(id=0, name=\"ab\", tags=[])\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = SentimentClassifier(INPUT_DIM, HIDDEN_DIM, OUTOUT_DIM, DROPOUT_RATE)\n",
    "load_checkpoint(model, filename=MODEL_FILE)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = tokenizers.Tokenizer.from_file(TOKENIZER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a new review\n",
    "review = \"This is a great movie! I love it.\"\n",
    "label, probability = predict_sentiment(review, model, tokenizer)\n",
    "print_prediction(review, label, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the colab environment does not support running a web server, we will use the function below to simulate the API call.\n",
    "def get_sentiment_api(request: dict[str:str]) -> dict:\n",
    "    \"\"\"\n",
    "    Get the sentiment prediction for a given review.\n",
    "    Args:\n",
    "        request (dict): A dictionary containing the review text.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the predicted sentiment and probability.\n",
    "    \"\"\"\n",
    "    review = request.get(\"review\")\n",
    "    label, probability = predict_sentiment(review, model, tokenizer)\n",
    "    sentiment = \"positive\" if label == 1 else \"negative\"\n",
    "    return {\"sentiment\": sentiment, \"probability\": probability}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the API function\n",
    "request = {\"review\": \"This is a great movie! I love it.\"}\n",
    "result = get_sentiment_api(request=request)\n",
    "print(f\"API Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the \"API\" handles an invalid input\n",
    "try:\n",
    "    get_sentiment_api({\"review\": 123})\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01: Input validation\n",
    "Your task is to validate the request dictionary within the api-call-function using a pydantic model.\n",
    "1. Create a Pydantic model named *SentimentRequest*.\n",
    "2. Enforce that the \"review\" field exists and is a string.\n",
    "3. Add basic validation: the review text should not be empty.\n",
    "4. Use the pydantic model within the api-call to validate the incoming request dictionary.\n",
    "\n",
    "**Hints**:\n",
    "* Use pydantics [BaseModel](\"https://docs.pydantic.dev/latest/api/base_model/\") and [Field](\"https://docs.pydantic.dev/latest/concepts/fields/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the pydantic model \"SentimentRequest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the function below to use the pydantic model to validate the request\n",
    "def get_sentiment_api(request: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Get the sentiment prediction for a given review.\n",
    "    Args:\n",
    "        request (dict): A dictionary containing the review text.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the predicted sentiment and probability.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Create an instance of SentimentRequest using the request data\n",
    "    sentiment_request = \n",
    "    review = sentiment_request.review\n",
    "    \n",
    "    label, probability = predict_sentiment(review, model, tokenizer)\n",
    "    sentiment = \"positive\" if label == 1 else \"negative\"\n",
    "    return {\"sentiment\": sentiment, \"probability\": probability}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the API with invalid inputs. The error message should now be clear and informative.\n",
    "request = {\"review\": 123}\n",
    "try:\n",
    "    get_sentiment_api(request)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
