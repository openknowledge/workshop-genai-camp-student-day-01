{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePcF01H6rr16"
   },
   "source": [
    "# GenAI-Camp: Day 01\n",
    "## Lesson: Data Preparation\n",
    "\n",
    "This lesson is intended to show you the basics of data preparation using python.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- remove incomplete data\n",
    "- remove html tags\n",
    "- remove stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxQu4eGGDrJe"
   },
   "source": [
    "### Set up the environment\n",
    "Import the necessary libraries, set constants, and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check runtime environment to make sure we are running in a colab environment. \n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\") \n",
    "else:\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of ressources\n",
    "if COLAB:\n",
    "    # Clone the data repository into colab\n",
    "    !git clone https://github.com/openknowledge/workshop-genai-camp-data.git\n",
    "    DATA_PATH = \"/content/workshop-genai-camp-data/day-01/data\"\n",
    "else:\n",
    "    DATA_PATH = \"../data\"\n",
    "IMDB_FILE = DATA_PATH + \"/imdb_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "df = pd.read_csv(IMDB_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first prepocessing step is to rename the columns\n",
    "def rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename columns of the dataframe to make them more readable.\n",
    "    \"\"\"\n",
    "\n",
    "    df_renamed = df.copy()\n",
    "    df_renamed.rename(columns={\"text\": \"review\", \"label\": \"sentiment\"}, inplace=True)\n",
    "\n",
    "    return df_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the result\n",
    "df = rename_columns(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01: Remove punctuations\n",
    "Since we want to remove all characters, which might be useless for classification, we remove punctuations. Keep in mind: We did not test if punctuations correlate to any kind of sentiment. In a real world scenario, we should analyse this first.\n",
    "1. Look at `string.punctuation`.\n",
    "2. Update the function below, to remove all punctuations\n",
    "\n",
    "**Hints**:\n",
    "* There are several ways to archieve this. One way ist to use `str.translate` (see [additional information](https://www.geeksforgeeks.org/python-string-translate/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Have a closer look at string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the function to remove punctuation marks\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove punctuation from the text.\n",
    "    \"\"\"\n",
    "    # TODO: Create a translation table\n",
    "    translation_table = \n",
    "    \n",
    "    return text.translate(translation_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test removing punctuation\n",
    "text = \"Hello, world! This is a test.\"\n",
    "print(remove_punctuation(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 02: Remove HTML-Tags\n",
    "In some text datasets, like IMDB reviews, HTML tags (`<p>`, `<br>`, `<div>`, etc.) are included. These tags are irrelevant for sentiment analysis and should be removed to clean the data and improve readability. In this exercise, you will implement a function that removes HTML tags from the reviews.\n",
    "1. Implement a function to remove the HTML tags\n",
    "\n",
    "**Hints**:\n",
    "* Use the library [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) from the bs4 module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove HTML tags from the text argument\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML tags from a string.\n",
    "    \"\"\"\n",
    "    # TODO: Use BeautifulSoup to remove HTML tags and return the cleaned text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test removing HTML tags\n",
    "html_text = \"This is a <b>sample</b> review with HTML tags\"\n",
    "print(remove_html_tags(html_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 03: Lowercasing the Reviews\n",
    "In text processing, it is common to convert all characters to lowercase. This helps to avoid treating words like \"Good\" and \"good\" as different tokens. In this exercise, you will implement a function that converts all text in the reviews to lowercase.\n",
    "1. Implement a function to transform text to lowercase\n",
    "\n",
    "**Hints**:\n",
    "* See `hello-python` notebook for string operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a function to transform a string to lowercase\n",
    "def transform_to_lowercase(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Transform a string to lowercase.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Transform the text to lowercase and return it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test transforming the text to lowercase\n",
    "text = \"This is a SAMPLE review\"\n",
    "print(transform_to_lowercase(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 04: Remove stopwords\n",
    "Stop words are common words (like \"the,\" \"is,\" \"in,\" \"and\") that often do not contribute much meaning to the analysis of text. Removing them can help reduce the noise in the data and improve the performance of models. \n",
    "1. Implement a function to remove stop words from a text.\n",
    "2. Use the nltk library\n",
    "3. Use the stopwords for filtering the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# TODO: Update the function to remove stopwords\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove stopwords from a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the english stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # TODO: Split the text into words\n",
    "    words = \n",
    "\n",
    "    # TODO: Filter words that are not in the stopwords. Use i.e. a for loop.\n",
    "    filtered_text = \n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the removing of stopwords. Try different capitalizations. What do you conclude?\n",
    "text = \"This is a sample review with some stopwords\"\n",
    "print(remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 05: Stemming\n",
    "Stemming is a crucial preprocessing step in Natural Language Processing (NLP) that reduces words to their root or base form. This normalization helps to group similar words and reduces dimensionality, which can improve the performance and generalization of machine learning models. It also helps to avoid treating different forms of a word as separate entities, making text analysis more effective.\n",
    "- Update the function below to stem an input string\n",
    "- Use the `nltk` library\n",
    "- See [documentation](https://www.nltk.org/howto/stem.html) for usage of a stemmer\n",
    "\n",
    "**Hints**:\n",
    "* The stemmer stems words (not sentences), so you need to split the text into words first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the function to stem the words\n",
    "def stem_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Stem the text using a nltk stemmer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the stemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    # TODO: Split the text into words\n",
    "\n",
    "    # TODO: Stem each word\n",
    "\n",
    "    # TODO: Join the stemmed words back into a string and return it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the stemming function\n",
    "text = \"make made making makes\"\n",
    "print(stem_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Exercise 06: Data Preparation Pipeline\n",
    "In this exercise you should use all the preparation functions you implemented in the exercises above to create a preparation pipeline. Here, the pipeline is just another function, which uses the functions above. After preparing the data, visualize the word clouds (or bar plots showing the most common words) for each sentiment again. Do you see any improvements?\n",
    "1. Implement a function, which receives the original dataframe as an argument and returns the preprocessed dataframe\n",
    "2. Visualize Wordclouds or bar plots of most common words per sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your implementation of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your implementation to show the wordcloud or bar plot"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
