{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePcF01H6rr16"
   },
   "source": [
    "# GenAI-Camp: Day 01\n",
    "## Lesson: Modeling\n",
    "\n",
    "This lesson is intended to show you the basics of modeling using python.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- prepare data for model training\n",
    "- train a simple neural network using pytorch\n",
    "- visualize model performance metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxQu4eGGDrJe"
   },
   "source": [
    "### Set up the environment\n",
    "Import the necessary libraries, set constants, and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tokenizers\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check runtime environment to make sure we are running in a colab environment. \n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\") \n",
    "else:\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of ressources\n",
    "if COLAB:\n",
    "    # Clone the data repository into colab\n",
    "    !git clone https://github.com/openknowledge/workshop-genai-camp-data.git\n",
    "    ROOT_PATH = \"/content/workshop-genai-camp-data/day-01/\"\n",
    "else:\n",
    "    ROOT_PATH = \"../\"\n",
    "DATA_PATH = ROOT_PATH + \"/data\"\n",
    "MODEL_PATH = ROOT_PATH + \"/models\"\n",
    "\n",
    "IMDB_FILE = DATA_PATH + \"/imdb_dataset.csv\"\n",
    "TOKENIZER_FILE = MODEL_PATH + \"/tokenizer.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation functions\n",
    "\n",
    "def rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename columns of the dataframe to make them more readable.\n",
    "    \"\"\"\n",
    "\n",
    "    df_renamed = df.copy()\n",
    "    df_renamed.rename(columns={\"text\": \"review\", \"label\": \"sentiment\"}, inplace=True)\n",
    "\n",
    "    return df_renamed\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove punctuation from the text.\n",
    "    \"\"\"\n",
    "\n",
    "    translation_table = {ord(i): None for i in string.punctuation}\n",
    "    \n",
    "    return text.translate(translation_table)\n",
    "\n",
    "\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove HTML tags from a string.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def transform_to_lowercase(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Transform a string to lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove stopwords from a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the english stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    words = text.split()\n",
    "\n",
    "    filtered_text = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "\n",
    "def stem_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Stem the text using a nltk stemmer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the stemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Stem each word\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "\n",
    "def preparation_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the DataFrame for modeling using the available preprocessing functions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy dataframe\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    df_processed['review'] = df_processed['review'].apply(remove_html_tags)\n",
    "\n",
    "    # Remove punctuations\n",
    "    df_processed['review'] = df_processed['review'].apply(remove_punctuation)\n",
    "    \n",
    "    # Transform to lowercase\n",
    "    df_processed['review'] = df_processed['review'].apply(transform_to_lowercase)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    df_processed['review'] = df_processed['review'].apply(remove_stopwords)\n",
    "\n",
    "    # Stem the text\n",
    "    df_processed['review'] = df_processed['review'].apply(stem_text)\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for model training\n",
    "Even after completing the preprocessing pipeline, further data preparation steps are essential to optimize the performance of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset\n",
    "df = pd.read_csv(IMDB_FILE)\n",
    "\n",
    "# Run the preprocessing pipeline\n",
    "df = rename_columns(df)\n",
    "df_processed = preparation_pipeline(df)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "In order for neural networks to process text, it must be converted into numerical format. Tokenization is the process of splitting text into individual words or subwords and mapping them to unique integers or embeddings. This structured format enables neural networks to understand and learn from the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our corpus is defined by the available data\n",
    "corpus = df_processed['review'].to_list()\n",
    "\n",
    "# Define some constants\n",
    "UNKNOWN_TOKEN = '[UNK]'\n",
    "CONTINUING_SUBWORD_PREFIX = '##'\n",
    "VOCABULARY_SIZE = 2000\n",
    "\n",
    "# Change this if you want to train the tokenizer yourself.\n",
    "# Since this might take a while, we will use a pretrained tokenizer\n",
    "# to save time.\n",
    "TRAIN_TOKENIZER = False\n",
    "\n",
    "if TRAIN_TOKENIZER:\n",
    "    # Load a Byte-Pair Encoder for tokenization\n",
    "    tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE(\n",
    "        unk_token=UNKNOWN_TOKEN,\n",
    "        continuing_subword_prefix=CONTINUING_SUBWORD_PREFIX,\n",
    "    ))\n",
    "\n",
    "    # Set up a trainer for the tokenizer\n",
    "    trainer = tokenizers.trainers.BpeTrainer(\n",
    "        vocab_size = VOCABULARY_SIZE,\n",
    "        special_tokens=[UNKNOWN_TOKEN],\n",
    "    )\n",
    "\n",
    "    # Train the tokenizer on the corpus\n",
    "    tokenizer.train_from_iterator(corpus, trainer)\n",
    "else:\n",
    "    # Load a pretrained tokenizer\n",
    "    tokenizer = tokenizers.Tokenizer.from_file(TOKENIZER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tokenization\n",
    "text = \"this is a test sentence!\"\n",
    "encoding = tokenizer.encode(text)\n",
    "print(f\"Tokens: \\t{encoding.tokens}\")\n",
    "print(f\"Tokens-Ids:\\t{encoding.ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "Tokenized text needs to be converted into numerical vectors for the model to process. In addition, the neural network we will use later, needs a fixed input dimension. This can be done using a technique called \"Bag of Words\" (BoW), which represents text by the frequency of words, or here tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text: str, tokenizer: tokenizers.Tokenizer):\n",
    "  \"\"\"\n",
    "  Vectorizes the input text using a tokenizer.\n",
    "  Args:\n",
    "      text (str): The input text to be vectorized.\n",
    "      tokenizer (tokenizers.Tokenizer): The tokenizer to use for vectorization.\n",
    "  Returns:\n",
    "      np.ndarray: A vector representation of the input text.\n",
    "  \"\"\"\n",
    "  # Tokenize the text\n",
    "  encoding = tokenizer.encode(text)\n",
    "\n",
    "  # Create a vector of zeros with the size of the vocabulary\n",
    "  vector = np.zeros(VOCABULARY_SIZE)\n",
    "\n",
    "  # Bag Of Words: Count the number of times each token appears in the text\n",
    "  for token_id in encoding.ids:\n",
    "      vector[token_id] +=1\n",
    "\n",
    "  # Normalize the vector\n",
    "  vector = vector / vector.sum()\n",
    "  return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the vectorization\n",
    "text = \"this is a test sentence!\"\n",
    "vector = vectorize_text(text, tokenizer)\n",
    "print(f\"Length of the vector: {len(vector)}\")\n",
    "print(f\"Vector: {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "To evaluate the performance of the model, the data must be split into training and test sets. This allows the model to learn patterns on the training set and be validated on unseen data from the test set, ensuring that it generalizes well and is not overfitting. A typical split is 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "df_train, df_test = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Show some information about the split\n",
    "print(f\"Number of training samples: {len(df_train)}\")\n",
    "print(f\"Number of test samples: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the splitted data\n",
    "train_vectors = []\n",
    "for text in df_train['review']:\n",
    "  train_vectors.append(vectorize_text(text, tokenizer))\n",
    "\n",
    "\n",
    "test_vectors = []\n",
    "for text in df_test['review']:\n",
    "  test_vectors.append(vectorize_text(text, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example of the vectorization\n",
    "print(f\"Vectorized training sample: {train_vectors[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Once the data is fully preprocessed and prepared, the next step is model training. In this phase, a machine learning model, such as a neural network, learns patterns and relationships from the training data to make predictions.  \n",
    "*Neural networks* are powerful models inspired by the human brain, consisting of layers of interconnected neurons. Each neuron receives input, processes it, and passes the result to the next layer. The network adjusts its internal parameters during training to minimize the error between its predictions and the actual results.  \n",
    "During the training process, several key settings, known as *hyperparameters*, need to be configured:\n",
    "* *Learning Rate*: Controls how much the model adjusts in response to errors.\n",
    "* *Batch Size*: Defines the number of samples processed before updating the model.\n",
    "* *Number of Epochs*: Determines how many times the entire training dataset is passed through the network.\n",
    "* *Number of Layers and Neurons*: Specifies the structure of the network, affecting its capacity to learn complex patterns.\n",
    "\n",
    "Unlike model parameters, which are learned during training, hyperparameters must be set before the learning process begins. Finding the optimal configuration often requires experimentation and techniques like Grid Search or Random Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "INPUT_DIM = VOCABULARY_SIZE\n",
    "OUTOUT_DIM = 2\n",
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 0.0003\n",
    "DROPOUT_RATE = 0.7\n",
    "NUM_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the torch library to create our model. Therefore, we need to convert our data into torch tensors.\n",
    "# Convert vectors\n",
    "train_vectors = torch.tensor(train_vectors, dtype=torch.float32)\n",
    "test_vectors = torch.tensor(test_vectors, dtype=torch.float32)\n",
    "\n",
    "# Convert labels\n",
    "train_labels = torch.tensor(df_train['sentiment'].values, dtype=torch.long)\n",
    "test_labels = torch.tensor(df_test['sentiment'].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for the training and test data\n",
    "train_dataset = TensorDataset(train_vectors, train_labels)\n",
    "test_dataset = TensorDataset(test_vectors, test_labels)\n",
    "\n",
    "# Create a DataLoader for the training and test data\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.linear_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.drop_out_1 = nn.Dropout(p=dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop_out_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper class for the training and evaluation process\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for training and evaluating a PyTorch model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, train_loader: DataLoader, test_loader: DataLoader,\n",
    "                 criterion: nn.Module, optimizer: optim.Optimizer, num_epochs: int):\n",
    "        \"\"\" \n",
    "        Trainer class for training and evaluating a PyTorch model.\n",
    "        Args:\n",
    "            model (nn.Module): The model to be trained.\n",
    "            train_loader (DataLoader): DataLoader for the training data.\n",
    "            test_loader (DataLoader): DataLoader for the test data.\n",
    "            criterion (nn.Module): Loss function.\n",
    "            optimizer (optim.Optimizer): Optimizer for the model.\n",
    "            num_epochs (int): Number of epochs to train the model.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.num_epochs = num_epochs\n",
    "        self.train_accuracies = []\n",
    "        self.test_accuracies = []\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        for inputs, labels in self.train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def evaluate(self, loader) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model on the given DataLoader.\n",
    "        Args:\n",
    "            loader (DataLoader): DataLoader for the evaluation data.\n",
    "        Returns:\n",
    "            float: Accuracy of the model on the evaluation data.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader:\n",
    "                outputs = self.model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model for a specified number of epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.train_epoch()\n",
    "            train_accuracy = self.evaluate(self.train_loader)\n",
    "            test_accuracy = self.evaluate(self.test_loader)\n",
    "\n",
    "            self.train_accuracies.append(train_accuracy)\n",
    "            self.test_accuracies.append(test_accuracy)\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{self.num_epochs}] '\n",
    "                  f'Train Accuracy: {train_accuracy:.2f}% '\n",
    "                  f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    def final_test_accuracy(self):\n",
    "        final_accuracy = self.evaluate(self.test_loader)\n",
    "        print(f'Final Accuracy on Test Data: {final_accuracy:.2f}%')\n",
    "        return final_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "\n",
    "# Create the model\n",
    "model = SentimentClassifier(INPUT_DIM, HIDDEN_DIM, OUTOUT_DIM, DROPOUT_RATE)\n",
    "\n",
    "# Create the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Instantiate the Trainer class\n",
    "trainer = Trainer(model, train_loader, test_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "trainer.final_test_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01: Plotting Model Accuracy Over Epochs\n",
    "Visualizing the training and testing accuracy of your model over time is crucial for understanding how well your model is learning. It can reveal if your model is:  \n",
    "* *Underfitting*: Both train and test accuracies are low.\n",
    "* *Overfitting*: Train accuracy is high, but test accuracy is low.\n",
    "* *Well-fitted*: Both accuracies improve and stabilize over epochs  \n",
    "\n",
    "Using Matplotlib, plot the training and testing accuracies stored in the Trainer class.\n",
    "\n",
    "**Hints**:\n",
    "* You can access i.e. the test accuracy by `trainer.test_accuracy`\n",
    "* Use a lineplot from *matplotlib*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the training and test accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 02: Model Inference\n",
    "After training a model, it's important to test its inference capabilities on new, unseen data. This allows you to verify if your model generalizes well beyond the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to predict the sentiment of a given text using the trained model.\n",
    "def predict_sentiment(text: str, model: nn.Module, tokenizer: tokenizers.Tokenizer) -> tuple[int, float]: # tuple[label, probability]\n",
    "    \"\"\"\n",
    "    Predict the sentiment of a given text using the trained model.\n",
    "    Args:\n",
    "        text (str): The text to be predicted.\n",
    "        model (nn.Module): The trained model.\n",
    "        tokenizer (tokenizers.Tokenizer): The tokenizer used for vectorization.\n",
    "    Returns:\n",
    "        int: The predicted sentiment (0 or 1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess the text\n",
    "    processed_text = preparation_pipeline(pd.DataFrame({'review': [text]}))['review'][0]\n",
    "\n",
    "    # Vectorize the text\n",
    "    vector = vectorize_text(processed_text, tokenizer)\n",
    "    vector = torch.tensor(vector, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # Predict the sentiment\n",
    "    with torch.no_grad():\n",
    "        output = model(vector)\n",
    "\n",
    "    label = torch.argmax(output, dim=1).item()\n",
    "    probability = output[0][label].item()\n",
    "\n",
    "    return (label, probability)\n",
    "\n",
    "def print_prediction(review: str, label: int, probability: float):\n",
    "    \"\"\"\n",
    "    Print the prediction result.\n",
    "    Args:\n",
    "        review (str): The review text.\n",
    "        label (int): The predicted sentiment (0 or 1).\n",
    "        probability (float): The probability of the predicted sentiment.\n",
    "    \"\"\"\n",
    "    sentiment = \"positive\" if label == 1 else \"negative\"\n",
    "    print(f\"Review: {review}\\nPredicted Sentiment: {sentiment} ({probability:.2f})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the prediction function with some example reviews\n",
    "review = \"This movie was fantastic! I loved it.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Bonus) Exercise 03: Saving and Restoring the Model from a Checkpoint\n",
    "When training deep learning models, it's crucial to save your progress and be able to restore it later. This allows you to:\n",
    "* *Resume Training*: Continue from where you left off.\n",
    "* *Model Deployment*: Load the trained model for inference.\n",
    "* *Experimentation*: Easily switch between different trained models.\n",
    "Your task is to create two standalone functions for:\n",
    "1. Saving the Model: Save the model's state to a checkpoint file.\n",
    "2. Loading the Model: Restore the model from that checkpoint file.\n",
    "\n",
    "**Hints**:\n",
    "* see [pytorch documentation](https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html) for examples of saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only save the model state, not the optimizer state\n",
    "# This is because we are not resuming training, but only using the model for inference\n",
    "# If we want to resume training, we need to save the optimizer state as well.\n",
    "def save_checkpoint(model: nn.Module, filename: str):\n",
    "    \"\"\"\n",
    "    Save the model state to a file.\n",
    "    Args:\n",
    "        model (nn.Module): The model to be saved.\n",
    "        filename (str): The name of the file to save the checkpoint to.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Model state has been saved to {filename}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model: nn.Module, filename: str):\n",
    "    \"\"\"\n",
    "    Load the model state from a file.\n",
    "    Args:\n",
    "        model (nn.Module): The model to be loaded.\n",
    "        filename (str): The name of the file to load the checkpoint from.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model state has been restored from {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save the model\n",
    "checkpoint_file_name = 'sentiment_model.pth'\n",
    "save_checkpoint(model=trainer.model, filename=checkpoint_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize a new model\n",
    "new_model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the sentiment of the review with the loaded model\n",
    "new_model_prediction, new_model_probability = predict_sentiment(text=review, model=new_model, tokenizer=tokenizer)\n",
    "print_prediction(review, new_model_prediction, new_model_probability)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
